{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, yaml, zipfile\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import glob\n",
        "import os, shutil\n",
        "from IPython.display import Image, display\n",
        "from utils.create_data_yaml import create_data_yaml_from_set_and_classes\n",
        "from utils.get_notebook_path import get_notebook_path\n",
        "from utils.clear_folder import clear_folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUfcA8ZgR2t"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook uses [Ultralytics](https://docs.ultralytics.com/) to train YOLO11, YOLOv8, or YOLOv5 object detection models with a custom dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NW7LLv_QPOO"
      },
      "source": [
        "**Verify NVIDIA GPU Availability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfaWho47RGDf",
        "outputId": "3939810c-b265-4c66-d2d3-2e512309453f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Nov 15 20:17:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1050 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
            "| 30%   25C    P8             N/A /   75W |     651MiB /   4096MiB |      8%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A       636    C+G   ...cw5n1h2txyewy\\CrossDeviceResume.exe      N/A      |\n",
            "|    0   N/A  N/A      1904    C+G   ...66.0_x64__8wekyb3d8bbwe\\Copilot.exe      N/A      |\n",
            "|    0   N/A  N/A      4812    C+G   ...gine\\app-4.0.562\\RazerAppEngine.exe      N/A      |\n",
            "|    0   N/A  N/A      5588    C+G   ...US\\ArmouryDevice\\asus_framework.exe      N/A      |\n",
            "|    0   N/A  N/A      7132    C+G   ...on\\wallpaper_engine\\wallpaper32.exe      N/A      |\n",
            "|    0   N/A  N/A     10128    C+G   C:\\Program Files\\Parsec\\parsecd.exe         N/A      |\n",
            "|    0   N/A  N/A     14328    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     14660    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
            "|    0   N/A  N/A     14876    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     15040    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
            "|    0   N/A  N/A     15336    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
            "|    0   N/A  N/A     15988    C+G   ...pcy8vm99wrpcg\\ModernFlyoutsHost.exe      N/A      |\n",
            "|    0   N/A  N/A     18372    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
            "|    0   N/A  N/A     18380    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     18872    C+G   ...on\\142.0.3595.80\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A     21048    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
            "|    0   N/A  N/A     22252    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
            "|    0   N/A  N/A     26856    C+G   D:\\Program Files\\cursor\\Cursor.exe          N/A      |\n",
            "|    0   N/A  N/A     27560    C+G   ...98.0_x64__zpdnekdrzrea0\\Spotify.exe      N/A      |\n",
            "|    0   N/A  N/A     29060    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPZEM27IOh79"
      },
      "source": [
        "**Option 1. Upload the Dataset**\n",
        "\n",
        "Upload the `data.zip` (Export from Label studio) to the root folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Iz9eBzW5zm"
      },
      "source": [
        "## 2. Split images into train and validation folders\n",
        "Next, we'll unzip `data.zip` and create some folders to hold the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z8O6z-wVcPEF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset/set.zip not found; skipping set extraction.\n"
          ]
        }
      ],
      "source": [
        "# Extract custom_data (required)\n",
        "extract_dir = \"dataset/custom_data\"\n",
        "zip_path = \"dataset/data.zip\"\n",
        "\n",
        "clear_folder(extract_dir)\n",
        "\n",
        "if os.path.exists(zip_path):\n",
        "    clear_folder(extract_dir)\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "else:\n",
        "    print(\"dataset/set.zip not found; skipping set extraction.\")\n",
        "\n",
        "# Extract set (optional)\n",
        "set_extract_dir = \"dataset/set\"\n",
        "set_zip_path = \"dataset/set.zip\"\n",
        "\n",
        "if os.path.exists(set_zip_path):\n",
        "    clear_folder(set_extract_dir)\n",
        "    os.makedirs(set_extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(set_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(set_extract_dir)\n",
        "else:\n",
        "    print(\"dataset/set.zip not found; skipping set extraction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoPjqW6AYebn"
      },
      "source": [
        "Now we will build the Ultralytics folder structure under `data/` by merging:\n",
        "\n",
        "- `dataset/set/` (from `set.zip`): copy `train` and `valid` (and optionally `test`) into `data/`.\n",
        "- `dataset/custom_data/`: split 80/20 (configurable) and append into `data/train` and `data/validation`.\n",
        "\n",
        "The script avoids filename collisions and pairs each image with its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8X62eFTugosf"
      },
      "outputs": [],
      "source": [
        "clear_folder(\"data\")\n",
        "!python -m utils.merge_datasets --set_dir dataset/set --custom_dir dataset/custom_data --data_dir data --train_pct 0.9 --include_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZoMkSFN9XG"
      },
      "source": [
        "# 4.&nbsp;Configure Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5Kdh0GmQHS"
      },
      "source": [
        "There's one last step before training: create `data.yaml`. We'll take the class names (`names` and `nc`) directly from `dataset/set/data.yaml`, and point `train`/`val` to our merged `data/` folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4letvP7X12ji"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classes.txt not found at dataset/custom_data/classes.txt\n",
            "Created config file at data.yaml from union of dataset/set/data.yaml and dataset/custom_data/classes.txt\n",
            "\n",
            "File contents:\n",
            "\n",
            "path: data\n",
            "train: train/images\n",
            "val: validation/images\n",
            "nc: 1\n",
            "names:\n",
            "- person\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path_to_set_yaml = 'dataset/set/data.yaml'\n",
        "path_to_classes_txt = 'dataset/custom_data/classes.txt'\n",
        "path_to_data_yaml = 'data.yaml'\n",
        "\n",
        "create_data_yaml_from_set_and_classes(path_to_set_yaml, path_to_classes_txt, path_to_data_yaml)\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "with open(\"data.yaml\", \"r\") as f:\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myP80_bnTNMi"
      },
      "source": [
        "# 5.&nbsp;Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfKspYasCzC8"
      },
      "source": [
        "## 5.1 Training Parameters\n",
        "\n",
        "**Model architecture & size (`model`):**\n",
        "\n",
        "There are several YOLO11 models sizes available to train, including `yolo11n.pt`, `yolo11s.pt`, `yolo11m.pt`, `yolo11l.pt`, and `yolo11xl.pt`. Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. \n",
        "\n",
        "Example: [check it out here to get a sense of their speed accuracy](https://youtu.be/_WKS4E9SmkA). \n",
        "By default, `yolo11s.pt` will be used as the starting point.\n",
        "\n",
        "We can also train with YOLOv8 or YOLOv5 models by substituting `yolo11` for `yolov8` or `yolov5`.\n",
        "\n",
        "\n",
        "**Number of epochs (`epochs`)**\n",
        "In machine learning, one “epoch” is one single pass through the full training dataset. Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset and the model architecture. \n",
        "\n",
        "In our case we will use 60 epochs. \n",
        "If your dataset has more than 200 images, a good starting point is 40 epochs.\n",
        "\n",
        "\n",
        "**Resolution (`imgsz`)**\n",
        "Resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. \n",
        "YOLO models are typically trained and inferenced at a 640x640 resolution. \n",
        "\n",
        "We can choose to lower the resolution to 480x480 for better model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQi_hXnUVPr-"
      },
      "source": [
        "model = Specifies the model to use\n",
        "epochs = Specifies the number of training epochs\n",
        "imgsz = Specifies the resolution of the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "NVIDIA GeForce GTX 1050 Ti\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detectada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bbpob1gTPlo"
      },
      "outputs": [],
      "source": [
        "clear_folder(\"runs/detect/train\")\n",
        "\n",
        "model = YOLO(\"yolo11s.pt\")\n",
        "\n",
        "model.train(\n",
        "    data=\"data.yaml\",\n",
        "    epochs=60,\n",
        "    imgsz=640\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv0EYWJ5V6mC"
      },
      "source": [
        "The training algorithm will parse the images in the training and validation directories and then start training the model. \n",
        "\n",
        "At the end of each training epoch, the program runs the model on the validation dataset and reports the resulting mAP, precision, and recall. As training continues, the mAP should generally increase with each epoch. Training will end once it goes through the number of epochs specified.\n",
        "\n",
        "The best trained model weights will be saved in `content/runs/detect/train/weights/best.pt`. Additional information about training is saved in the `content/runs/detect/train` folder, including a `results.png` file that shows how loss, precision, recall, and mAP progressed over each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6.&nbsp;Test Model\n",
        "The commands below run the model on the images in the validation folder and then display the results for the first 10 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PooP5Vjsg2Jn"
      },
      "outputs": [],
      "source": [
        "clear_folder(\"runs/detect/predict\")\n",
        "\n",
        "model = YOLO(\"runs/detect/train3/weights/best.pt\")\n",
        "\n",
        "results = model.predict(\n",
        "    source=\"data/validation/images\",  \n",
        "    save=True,\n",
        "    show=False,\n",
        "    conf=0.25\n",
        ")\n",
        "\n",
        "for image_path in glob.glob(f'runs/detect/predict/*.jpg')[:5]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGiQw_gWbSBa"
      },
      "source": [
        "The model should draw a box around each object of interest in each image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7yrFRViVczX"
      },
      "source": [
        "# 7.&nbsp;Deploy Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcoBAeHXa86W"
      },
      "source": [
        "## 7.1 Download YOLO Model\n",
        "\n",
        "First, zip and download the trained model by running the code blocks below.\n",
        "\n",
        "The code creates a folder named `my_model`, moves the model weights into it, and renames them from `best.pt` to `my_model.pt`, and also adds the training results for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcBdnOA9v85S"
      },
      "outputs": [],
      "source": [
        "#clear_folder(\"my_model\")\n",
        "\n",
        "base_dir = get_notebook_path()\n",
        "train_dir = os.path.join(base_dir, \"runs\", \"detect\", \"train3\")\n",
        "output_dir = os.path.join(base_dir, \"my_model\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "shutil.copy(os.path.join(train_dir, \"weights\", \"best.pt\"), os.path.join(output_dir, \"my_model.pt\"))\n",
        "shutil.copytree(train_dir, os.path.join(output_dir, \"train\"), dirs_exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Using the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python utils/yolo_detect.py --model my_model/my_model.pt --source usb1 --resolution 1280x720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python utils/yolo_detect.py --model my_model/persons_model.pt --source usb1 --resolution 1280x720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python utils/yolo_detect.py --model my_model/full_model.pt --source usb1 --resolution 1280x720"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "train-model-env1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
